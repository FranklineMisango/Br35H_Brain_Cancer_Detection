{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VGG16 model development and fine tuning\n",
    "\n",
    "This Notebook loads and adds our layers to the base model of the pre-trained VGG16 model, and fine tune the following components of the model:\n",
    "1. **Optimizers** - We will use only 5 optimizers to see which one works best, namely; 1. `SGD`,2. `RMSprop`,3. `Adam`,4. `Adagrad`,5. `Adadelta`. the best optimizer will be used in the New model\n",
    "2. **Number of epochs** - 4 epochs to choose the best performer namely; `1, 2, 5, 10`.\n",
    "3. **Batch size** - Batch sizes ; `8, 16`. \n",
    "4. **Dropout value** - Dropout values : `0.5, 0.6, 0.7, 0.8, 0.9`\n",
    "\n",
    "> **Section one:** Following initial parameters: **Batch size** `8`, **Optimizer** `Adam`, **Number of epochs** `5`, \n",
    "\n",
    "# **Import libraries that we will use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 \n",
    "from tensorflow.keras.preprocessing import image \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Input\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import tqdm.gui as tqdm\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(train_data_path, \\\n",
    "                       val_data_path, \\\n",
    "                       targetsize, \\\n",
    "                       classmode, \\\n",
    "                       batchsize):\n",
    "    \"\"\"\n",
    "    This function is a data generator function for train, validation\n",
    "    Inputs\n",
    "        train_data_path   : train data path for the dataset \n",
    "        val_data_path     : validation data path for the dataset\n",
    "        targetsize        : target size for the generator to resize all images to, (224,224)\n",
    "        classmode         : class mode, 'categorical'\n",
    "        batchsize         : batch size\n",
    "    Outputs\n",
    "        train_generator   : generated train data\n",
    "        val_generator   : generated train data\n",
    "    \n",
    "    \"\"\"\n",
    "    train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    \n",
    "    train_generator=train_datagen.flow_from_directory(train_data_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize,\n",
    "                                                 class_mode=classmode,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    val_generator=val_datagen.flow_from_directory(val_data_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize,\n",
    "                                                 class_mode=classmode,\n",
    "                                                 shuffle=True)\n",
    "    return train_generator,val_generator   \n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate_the_model(train_generator, \\\n",
    "                            val_generator, \\\n",
    "                            optimizer, \\\n",
    "                            epochs, \\\n",
    "                            dropout_value, \\\n",
    "                            TheModel):\n",
    "    '''\n",
    "    train the model, do predictions, and do evaluation and return the accuracy of the model\n",
    "    Inputs\n",
    "        train_generator : generated train data\n",
    "        val_generator   : generated validation data\n",
    "        optimizer       : the optimizer method used to compile the model\n",
    "        epochs          : the epochs of the model use to fit the model\n",
    "        TheModel        : The pretrained model loaded,\n",
    "    Outputs\n",
    "        accuracy        : The accuracy of the model\n",
    "    '''\n",
    "    # load pretrained model and add layers on top of the model\n",
    "    x = TheModel.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024,activation='relu')(x)\n",
    "    x = Dense(1024,activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_value)(x)\n",
    "    preds = Dense(4,activation='softmax')(x)\n",
    "    model = Model(inputs = TheModel.input,outputs=preds)\n",
    "    \n",
    "    # freeze base layers for training\n",
    "    for layer in TheModel.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    step_size_train=train_generator.n//train_generator.batch_size\n",
    "    # fit the model\n",
    "    r = model.fit_generator(generator=train_generator,\n",
    "                        validation_data=val_generator,\n",
    "                        steps_per_epoch=step_size_train,\n",
    "                        epochs=epochs)\n",
    "    # valuate the model\n",
    "    scores = model.evaluate(val_generator)\n",
    "    accuracy = scores[1]*100\n",
    "    \n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"Datasets\"\n",
    "number_of_images = {}\n",
    "\n",
    "#This code lists the Amounts of Healthy and Non-healthy Brain Tumor Datasets\n",
    "for dir in os.listdir(ROOT_DIR):\n",
    "  number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR,dir)))\n",
    "\n",
    "number_of_images.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gli_images=os.listdir(\"Datasets/Training/glioma_tumor/\")\n",
    "\n",
    "imgs=[]\n",
    "for i in range(len(Gli_images)):\n",
    "    img=load_img(Gli_images[i]) \n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=random.randint(1,len(imgs))\n",
    "im = img_to_array(imgs[z])\n",
    "print(\"Maximum pixel value before preprocessing: \",im.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
